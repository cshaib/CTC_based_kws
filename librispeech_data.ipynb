{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import librosa \n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhonemeTokenizer:\n",
    "# tokenize phonemes into IDs  \n",
    "    def __init__(self, phoneme_to_phoneme_index):\n",
    "        self.phoneme_to_phoneme_index = phoneme_to_phoneme_index\n",
    "        self.phoneme_index_to_phoneme = {v: k for k, v in self.phoneme_to_phoneme_index.items()}\n",
    "\n",
    "    def EncodeAsIds(self, phoneme_string):\n",
    "        return [self.phoneme_to_phoneme_index[p] for p in phoneme_string.split()]\n",
    "\n",
    "    def DecodeIds(self, phoneme_ids):\n",
    "        return \" \".join([self.phoneme_index_to_phoneme[id] for id in phoneme_ids])\n",
    "    \n",
    "# label_set = [1: 'AA0', 'AA1', 'AA2', 'AE0', 'AE1', 'AE2', 'AH0', 'AH1', 'AH2', 'AO0', 'AO1', 'AO2', \\\n",
    "#              'AW0', 'AW1', 'AW2', 'AY0', 'AY1', 'AY2', 'EH0', 'EH1', 'EH2', 'ER0', 'ER1', 'ER2', \\\n",
    "#              'EY0', 'EY1', 'EY2', 'IH0', 'IH1', 'IH2', 'IY0', 'IY1', 'IY2', 'OW0', 'OW1', 'OW2', \\\n",
    "#              'OY0', 'OY1', 'OY2', 'UH0', 'UH1', 'UH2', 'UW0', 'UW1', 'UW2', \\\n",
    "#              'B', 'CH', 'D', 'DH', 'F', 'G', 'HH', 'JH', 'K', 'L', 'M', 'N', 'NG', 'P', 'R', \\\n",
    "#              'S', 'SH', 'T', 'TH', 'V', 'W', 'Y', 'Z', 'ZH', 'sil', 'sp', '']\n",
    "\n",
    "label_set = ['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G', 'HH',\n",
    "             'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH',\n",
    "             'UW', 'V', 'W', 'Y', 'Z', 'ZH', 'SIL', 'SPN', '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chantal/Desktop/systematic_review/abstract_env/lib/python3.7/site-packages/librosa/filters.py:239: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  \"Empty filters detected in mel frequency basis. \"\n"
     ]
    }
   ],
   "source": [
    "# load in audio data in train set \n",
    "\n",
    "audio_path = \"/Users/chantal/Desktop/StMichaels/unpacked/\"\n",
    "data = os.listdir(audio_path)\n",
    "\n",
    "label_path = \"/Users/chantal/Desktop/StMichaels/mini-librispeech-csv/train_data.csv\"\n",
    "\n",
    "all_data = {}\n",
    "\n",
    "# load in the audio and get mfcc features for each audio \n",
    "for item in data: \n",
    "    if item.endswith(\".flac\"):\n",
    "        y, sr = librosa.load(audio_path+item)\n",
    "        feat = librosa.feature.mfcc(y, sr, n_mfcc=30, n_fft=25, hop_length=10)\n",
    "        all_data[item[:-5]] = feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1519 1519 1519\n"
     ]
    }
   ],
   "source": [
    "# load in the labels\n",
    "label_df = pd.read_csv(label_path)\n",
    "phoneme_df = label_df['phonemes_39'].tolist()\n",
    "l = []\n",
    "n = []\n",
    "v = []\n",
    "\n",
    "for (key, value), label in zip(all_data.items(), phoneme_df):\n",
    "    int_label = [label_set.index(x.upper()) for x in label.split()]\n",
    "    l.append(int_label)\n",
    "    n.append(key)\n",
    "    v.append(value)\n",
    "    \n",
    "# save down data in pandas dataframe format: filename - data - labels for train, test, and dev. \n",
    "print(len(n), len(v), len(l))\n",
    "df = pd.DataFrame({'name': n, 'audio': v, 'label':l})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1000:].to_pickle(\"all_train_audio_2\",  compression='gzip', protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['audio'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('abstract_env': venv)",
   "language": "python",
   "name": "python37164bitabstractenvvenvf1745870c2884b50b41ace250daa77db"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
